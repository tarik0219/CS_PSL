{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pranav/workspace/UIUC/Fall2020/CS_PSL/Project_3\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1 = pd.read_csv('split_1/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Afraid of the Dark left me with the impression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>This has to be one of the biggest misfires eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>This is one of those movies I watched, and won...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>This movie was dreadful. Biblically very inacc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  sentiment                                             review\n",
       "0   1          1  Naturally in a film who's main themes are of m...\n",
       "1   4          0  Afraid of the Dark left me with the impression...\n",
       "2   7          0  This has to be one of the biggest misfires eve...\n",
       "3   8          0  This is one of those movies I watched, and won...\n",
       "4  17          0  This movie was dreadful. Biblically very inacc..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1['sentiment'] = split1['sentiment'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            int64\n",
       "sentiment     int64\n",
       "review       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Review cleanup (this process takes a significant amount of time)\n",
    "# Removing stop words, @ mentions, webpages and special characters\n",
    "\n",
    "from nltk.corpus import stopwords # nltk.download('stopwords') before importing\n",
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def clean(review):\n",
    "    \n",
    "    stage1 = [word for word in review.lower().split() if word not in stopwords.words('english')] # stopword removal\n",
    "    stage2 = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in stage1]\n",
    "    stage3 = [REPLACE_WITH_SPACE.sub(\" \", line) for line in stage2]\n",
    "    return ' '.join(stage3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 45s, sys: 2min 6s, total: 9min 51s\n",
      "Wall time: 9min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "split1['review_cleaned'] = split1['review'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1x = split1['review_cleaned']\n",
    "train_1y = split1['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 219 ms, total: 11.1 s\n",
      "Wall time: 11.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=1000, ngram_range=(1, 2), stop_words='english')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = TfidfVectorizer(max_features = 1000, ngram_range = (1, 2), stop_words = 'english')\n",
    "%time vector.fit(train_1x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.71 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(ngram_range=(1, 2), stop_words='english',\n",
       "                vocabulary=dict_keys(['naturally', 'film', 'main', 'themes', 'loss', 'surprising', 'rated', 'highly', 'older', 'viewers', 'younger', 'ones', 'enjoy', 'pace', 'constant', 'characters', 'engaging', 'relationships', 'natural', 'showing', 'need', 'tears', 'emotion', 'fear', 'violence', 'short', 'story', 'ready', 'perfect', 'sm..., 'joan', 'spanish', 'joseph', 'directing', 'gang', 'anthony', 'context', 'speech', 'dollars', 'br overall', 'heads', 'felt like', 'twist', 'priest', 'fault', 'sucks', 'batman', 'parody', 'featuring', 'eat', 'test', 'andy', 'thoughts', 'stone', 'danny', 'genuinely', 'pleasure', 'grand', 'halloween', 'author', 'allen', 'river', 'howard', 'corny', 'nonsense', 'alex', 'surely', 'jackson']))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = TfidfVectorizer(vocabulary = vocab, ngram_range = (1, 2), stop_words = 'english')\n",
    "%time vector.fit(train_1x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1x_transformed = vector.transform(train_1x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1427174 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_1x_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.2.1-py3-none-win_amd64.whl (86.5 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\koric1\\anaconda3\\lib\\site-packages (from xgboost) (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\koric1\\anaconda3\\lib\\site-packages (from xgboost) (1.18.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.2.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 1000, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(train_1x_transformed, train_1y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53025972\n",
      "Iteration 2, loss = 0.34571696\n",
      "Iteration 3, loss = 0.31803466\n",
      "Iteration 4, loss = 0.31125274\n",
      "Iteration 5, loss = 0.30764142\n",
      "Iteration 6, loss = 0.30563182\n",
      "Iteration 7, loss = 0.30391523\n",
      "Iteration 8, loss = 0.30264072\n",
      "Iteration 9, loss = 0.30127874\n",
      "Iteration 10, loss = 0.29993344\n",
      "Iteration 11, loss = 0.29800184\n",
      "Iteration 12, loss = 0.29663987\n",
      "Iteration 13, loss = 0.29442219\n",
      "Iteration 14, loss = 0.29246888\n",
      "Iteration 15, loss = 0.29066464\n",
      "Iteration 16, loss = 0.28807279\n",
      "Iteration 17, loss = 0.28532660\n",
      "Iteration 18, loss = 0.28328976\n",
      "Iteration 19, loss = 0.28058753\n",
      "Iteration 20, loss = 0.27724138\n",
      "Iteration 21, loss = 0.27360825\n",
      "Iteration 22, loss = 0.27096189\n",
      "Iteration 23, loss = 0.26694364\n",
      "Iteration 24, loss = 0.26264752\n",
      "Iteration 25, loss = 0.25877983\n",
      "Iteration 26, loss = 0.25445277\n",
      "Iteration 27, loss = 0.24981203\n",
      "Iteration 28, loss = 0.24506888\n",
      "Iteration 29, loss = 0.24006425\n",
      "Iteration 30, loss = 0.23429868\n",
      "Iteration 31, loss = 0.22869109\n",
      "Iteration 32, loss = 0.22325539\n",
      "Iteration 33, loss = 0.21657505\n",
      "Iteration 34, loss = 0.21059004\n",
      "Iteration 35, loss = 0.20365370\n",
      "Iteration 36, loss = 0.19649665\n",
      "Iteration 37, loss = 0.19071065\n",
      "Iteration 38, loss = 0.18263436\n",
      "Iteration 39, loss = 0.17596267\n",
      "Iteration 40, loss = 0.16838105\n",
      "Iteration 41, loss = 0.16042119\n",
      "Iteration 42, loss = 0.15290301\n",
      "Iteration 43, loss = 0.14533115\n",
      "Iteration 44, loss = 0.13807550\n",
      "Iteration 45, loss = 0.13145730\n",
      "Iteration 46, loss = 0.12384012\n",
      "Iteration 47, loss = 0.11669047\n",
      "Iteration 48, loss = 0.10976535\n",
      "Iteration 49, loss = 0.10281270\n",
      "Iteration 50, loss = 0.09618999\n",
      "Iteration 51, loss = 0.08999686\n",
      "Iteration 52, loss = 0.08422110\n",
      "Iteration 53, loss = 0.07812413\n",
      "Iteration 54, loss = 0.07315728\n",
      "Iteration 55, loss = 0.06829252\n",
      "Iteration 56, loss = 0.06348903\n",
      "Iteration 57, loss = 0.05847775\n",
      "Iteration 58, loss = 0.05432758\n",
      "Iteration 59, loss = 0.05078657\n",
      "Iteration 60, loss = 0.04698711\n",
      "Iteration 61, loss = 0.04387651\n",
      "Iteration 62, loss = 0.04043838\n",
      "Iteration 63, loss = 0.03722169\n",
      "Iteration 64, loss = 0.03480819\n",
      "Iteration 65, loss = 0.03214811\n",
      "Iteration 66, loss = 0.02991715\n",
      "Iteration 67, loss = 0.02765637\n",
      "Iteration 68, loss = 0.02559796\n",
      "Iteration 69, loss = 0.02400102\n",
      "Iteration 70, loss = 0.02214476\n",
      "Iteration 71, loss = 0.02060873\n",
      "Iteration 72, loss = 0.01926531\n",
      "Iteration 73, loss = 0.01790718\n",
      "Iteration 74, loss = 0.01678593\n",
      "Iteration 75, loss = 0.01574141\n",
      "Iteration 76, loss = 0.01465673\n",
      "Iteration 77, loss = 0.01367686\n",
      "Iteration 78, loss = 0.01288571\n",
      "Iteration 79, loss = 0.01212220\n",
      "Iteration 80, loss = 0.01135192\n",
      "Iteration 81, loss = 0.01074254\n",
      "Iteration 82, loss = 0.01010373\n",
      "Iteration 83, loss = 0.00952392\n",
      "Iteration 84, loss = 0.00894813\n",
      "Iteration 85, loss = 0.00847873\n",
      "Iteration 86, loss = 0.00803569\n",
      "Iteration 87, loss = 0.00764156\n",
      "Iteration 88, loss = 0.00723909\n",
      "Iteration 89, loss = 0.00689613\n",
      "Iteration 90, loss = 0.00660761\n",
      "Iteration 91, loss = 0.00626565\n",
      "Iteration 92, loss = 0.00599764\n",
      "Iteration 93, loss = 0.00573110\n",
      "Iteration 94, loss = 0.00548117\n",
      "Iteration 95, loss = 0.00525760\n",
      "Iteration 96, loss = 0.00506038\n",
      "Iteration 97, loss = 0.00484765\n",
      "Iteration 98, loss = 0.00467600\n",
      "Iteration 99, loss = 0.00449804\n",
      "Iteration 100, loss = 0.00432228\n",
      "Iteration 101, loss = 0.00418586\n",
      "Iteration 102, loss = 0.00403773\n",
      "Iteration 103, loss = 0.00391178\n",
      "Iteration 104, loss = 0.00377040\n",
      "Iteration 105, loss = 0.00364942\n",
      "Iteration 106, loss = 0.00353748\n",
      "Iteration 107, loss = 0.00343103\n",
      "Iteration 108, loss = 0.00332848\n",
      "Iteration 109, loss = 0.00323020\n",
      "Iteration 110, loss = 0.00317389\n",
      "Iteration 111, loss = 0.00305894\n",
      "Iteration 112, loss = 0.00297931\n",
      "Iteration 113, loss = 0.00291268\n",
      "Iteration 114, loss = 0.00283409\n",
      "Iteration 115, loss = 0.00277289\n",
      "Iteration 116, loss = 0.00272064\n",
      "Iteration 117, loss = 0.00266674\n",
      "Iteration 118, loss = 0.00261039\n",
      "Iteration 119, loss = 0.00257468\n",
      "Iteration 120, loss = 0.00253866\n",
      "Iteration 121, loss = 0.00250296\n",
      "Iteration 122, loss = 0.00246896\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(verbose=2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "NN = MLPClassifier(verbose=2)\n",
    "NN.fit(train_1x_transformed, train_1y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.fit(train_1x_transformed, train_1y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(train_1x_transformed, train_1y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(train_1x_transformed, train_1y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict(train_1x_transformed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08701922, 0.91298078]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict_proba(train_1x_transformed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.44162628, -0.09104045]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict_log_proba(train_1x_transformed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vector.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test it\n",
    "\n",
    "split1_test_x = pd.read_csv('split_1/test.tsv', sep = '\\t')\n",
    "split1_test_y = pd.read_csv('split_1/test_y.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1_test_x['review_cleaned'] = split1_test_x['review'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_id = split1_test_x['id'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  598, 12048, 40908, ...,  3620, 24858, 13068])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1x = vector.transform(split1_test_x['review_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1y = split1_test_y['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = rf.predict_proba(test_1x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_pred = NN.predict_proba(test_1x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = nb.predict(test_1x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1_prob = lg.predict_proba(test_1x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1_prob = xgb.predict_proba(test_1x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - 0.84548\n",
      "Confusion matrix - [[10374  2153]\n",
      " [ 1710 10763]]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy - {}'.format(accuracy_score(test_1y, pred1)))\n",
    "print('Confusion matrix - {}'.format(confusion_matrix(test_1y, pred1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred1_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"mysubmission.txt\",\"w\")\n",
    "file.write('\"id\"    \"prob\"')\n",
    "file.write(\"\\n\")\n",
    "num = len(split_id)\n",
    "for x,y in enumerate(split_id):\n",
    "    string = str(y) + \"    \"+ str(pred1_prob[x][1])\n",
    "    file.write(string)\n",
    "    if x != num - 1:\n",
    "        file.write(\"\\n\")\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"mysubmission_rf.txt\",\"w\")\n",
    "file.write('\"id\"    \"prob\"')\n",
    "file.write(\"\\n\")\n",
    "num = len(split_id)\n",
    "for x,y in enumerate(split_id):\n",
    "    string = str(y) + \"    \"+ str(rf_pred[x][1])\n",
    "    file.write(string)\n",
    "    if x != num - 1:\n",
    "        file.write(\"\\n\")\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"mysubmission_nn.txt\",\"w\")\n",
    "file.write('\"id\"    \"prob\"')\n",
    "file.write(\"\\n\")\n",
    "num = len(split_id)\n",
    "for x,y in enumerate(split_id):\n",
    "    string = str(y) + \"    \"+ str(nn_pred[x][1])\n",
    "    file.write(string)\n",
    "    if x != num - 1:\n",
    "        file.write(\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
